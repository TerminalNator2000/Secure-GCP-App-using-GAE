
### Example GCP/GAE Application

## **1. Set Up Your Google Cloud Project**

1. **Create a Google Cloud Project**: 
   - Go to the [Google Cloud Console](https://console.cloud.google.com/).
   - Create a new project or select an existing one.

2. **Enable Required APIs**:
   - Navigate to **APIs & Services** > **Library**.
   - Enable the following APIs:
     - **App Engine API**
     - **Cloud Pub/Sub API**
     - **BigQuery API**
     - **Cloud Dataflow API**

3. **Set Up Billing**: 
   - Ensure that billing is enabled for your project to avoid service interruptions.

---

## **2. Install Google Cloud SDK and Initialize Your Project**

Install the Google Cloud SDK if you haven’t already, and initialize it for your project.

```bash
# Install Google Cloud SDK (if not already installed)
curl https://sdk.cloud.google.com | bash

# Initialize SDK
gcloud init

# Set project ID
gcloud config set project YOUR_PROJECT_ID
```

---

## **3. Set Up Google App Engine**

```bash
# Create App Engine application
gcloud app create --region=YOUR_REGION
```

This command sets up the App Engine environment for your project.

---

## **4. Create the Application Files**

Create a simple Python web application for this example. Start by setting up your project directory and creating necessary files.

### File Structure:
```
example-app/
├── app.yaml
├── main.py
├── requirements.txt
```

### **`app.yaml`** - Configuration file for App Engine

```yaml
runtime: python39
entrypoint: python main.py

env_variables:
  PUBSUB_TOPIC: "projects/YOUR_PROJECT_ID/topics/data-topic"
  BIGQUERY_TABLE: "YOUR_PROJECT_ID.dataset.table"
```

Replace `YOUR_PROJECT_ID`, `dataset`, and `table` with your actual project ID, dataset, and table.

### **`requirements.txt`** - Dependencies

```txt
Flask==2.0.2
google-cloud-pubsub==2.7.0
google-cloud-bigquery==2.34.2
```

Install these dependencies locally by running:

```bash
pip install -r requirements.txt
```

### **`main.py`** - Core Application Logic

```python
from flask import Flask, request, jsonify
from google.cloud import pubsub_v1, bigquery
import os

app = Flask(__name__)

# Initialize Pub/Sub and BigQuery clients
pubsub_client = pubsub_v1.PublisherClient()
bigquery_client = bigquery.Client()

# Load environment variables
PUBSUB_TOPIC = os.getenv("PUBSUB_TOPIC")
BIGQUERY_TABLE = os.getenv("BIGQUERY_TABLE")

@app.route('/')
def index():
    return "Welcome to the Google App Engine Real-Time Analytics Application!"

@app.route('/publish', methods=['POST'])
def publish():
    data = request.json
    if not data:
        return jsonify({"error": "No data provided"}), 400
    
    # Publish data to Pub/Sub
    data_str = str(data)
    topic_path = pubsub_client.topic_path(os.getenv('GOOGLE_CLOUD_PROJECT'), PUBSUB_TOPIC.split('/')[-1])
    pubsub_client.publish(topic_path, data_str.encode("utf-8"))
    
    return jsonify({"message": "Data published to Pub/Sub"}), 200

@app.route('/process', methods=['POST'])
def process():
    data = request.json
    if not data:
        return jsonify({"error": "No data provided"}), 400

    # Insert data into BigQuery
    table = bigquery_client.get_table(BIGQUERY_TABLE)
    rows_to_insert = [data]
    errors = bigquery_client.insert_rows_json(table, rows_to_insert)

    if errors:
        return jsonify({"error": "Error inserting into BigQuery", "details": errors}), 500

    return jsonify({"message": "Data processed and stored in BigQuery"}), 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

---

## **5. Deploy the Application to App Engine**

Use the following command to deploy your application to Google App Engine:

```bash
gcloud app deploy
```

Once deployed, you can access your app’s endpoint at `https://YOUR_PROJECT_ID.ue.r.appspot.com/`.

---

## **6. Set Up Pub/Sub and BigQuery**

### **Create a Pub/Sub Topic**

```bash
gcloud pubsub topics create data-topic
```

### **Set Up BigQuery**

1. Go to [BigQuery Console](https://console.cloud.google.com/bigquery).
2. Create a new dataset (e.g., `realtime_analytics`).
3. Create a table within the dataset (e.g., `events`), and define the schema based on the type of data you expect to process.

For example:
   - `id` (STRING)
   - `event_name` (STRING)
   - `event_time` (TIMESTAMP)
   - `data` (STRING)

---

## **7. Configure Cloud Dataflow for Real-Time Data Processing**

For real-time data processing, you can use **Cloud Dataflow** to transform and route data from Pub/Sub to BigQuery.

1. Create a Dataflow job that subscribes to your Pub/Sub topic and streams the data into BigQuery.
2. Use the Dataflow template `PubSub to BigQuery` from the Dataflow templates in the Google Cloud Console.

### Configure the Dataflow job:
   - **Input Pub/Sub topic**: `projects/YOUR_PROJECT_ID/topics/data-topic`
   - **BigQuery table**: `YOUR_PROJECT_ID:realtime_analytics.events`

---

## **8. Test the Application**

1. **Publish Data to Pub/Sub**:
   Use a tool like Postman to send a `POST` request to the `/publish` endpoint with JSON data, for example:

   ```json
   POST https://YOUR_PROJECT_ID.ue.r.appspot.com/publish
   {
     "id": "12345",
     "event_name": "user_signup",
     "event_time": "2024-10-31T12:00:00Z",
     "data": "sample data"
   }
   ```

2. **Verify Data in BigQuery**:
   - Check your BigQuery table to ensure the data is processed and stored correctly.

---

## **9. Set Up Monitoring and Alerts**

1. **Enable Google Cloud Logging and Monitoring**:
   - Go to **Cloud Monitoring** in the Google Cloud Console and create dashboards to monitor the health and activity of your application.

2. **Set Alerts**:
   - Configure alerts for Pub/Sub error rates, App Engine latency, or BigQuery processing errors.

---

## **10. Continuous Integration & Deployment (Optional)**

1. Set up a CI/CD pipeline to automatically test and deploy updates to your application using **Cloud Build** or another CI/CD tool.

2. Automate security scans and checks in the pipeline to maintain security standards.

---

## Summary

This example demonstrates a secure, scalable application on Google App Engine, leveraging real-time data processing with Pub/Sub, Dataflow, and BigQuery. This setup enables you to capture, process, and analyze data in near real-time, aligning with best practices for secure cloud application development.

By following these steps, you’ll have a foundation for developing a robust application that meets real-time analytics needs while ensuring security and scalability.